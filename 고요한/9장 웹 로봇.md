# 9장 웹 로봇

- 웹 로봇은 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램이다.
- 많은 로봇이 웹사이트에서 다른 웹사이트로 떠돌아다니면서, 콘텐츠를 가져오고, 하이퍼링크를 따라가고, 그들이 발견한 데이터를 처리한다.
- 웹 로봇들은 마치 스스로 마음을 가지고 있는 것처럼 자동으로 웹사이트들을 탐색하며, 그 방식에 따라 ‘클로러’ , ‘스파이더’ , ’웜’ , ‘봇’, 등 각양각색의 이름으로 불린다.

## 9.1 크롤러와 크롤링

- 웹 크롤러는 , 먼저 웹페이지를 한 개 가져오고, 그 다음 그 페이지가 가리키는 모든 웹페이지를 가져오고, 다시 그 페이지들이 가리키는 모든 웹 페이지들을 가져오는 이러한 일을 재귀적으로 반복하는 방식으로 웹을 순회하는 로봇이다.
- 웹 링크를 재귀적으로 따라가는 로봇을 크롤러 혹은 스파이더라고 부르는데 HTML 하이퍼링크들로 만들어진 웹을 따라 “기어다니기crawl)” 때문이다.
- 인터넷 검색엔진은 웹을 돌아다니면서 그들이 만나는 모든 문서를 끌어오기 위해 크롤러를 사용한다.

### 9.1.1 어디에서 시작하는가 : “루트 집합”

- 출발지점을 주어야 한다. 크롤러가 방문을 시작하는 URL들의 초기 집합은 루트 집합(root set)이라고 불린다.
- 최종적으로 모든 문서로 이어지게 되는 하나의 문서란 없다.(한번에 이동을 할 수 없는 페이지가 있다)
- 일반적으로 웹의 대부분을 커버 하기 위해 루트 집합에 너무 많은 페이지가 있을 필요가 없다.

### 9.1.2 링크 추출과 상대 링크 정상화

- 크롤러는 웹을 돌아다니면서 꾸준히 HTML 문서를 검색한다.
- 크롤러는 검색한 각페이지 안에 들어있는 URL 링크들을 파싱해서 크롤링한 페이지들의 목록에 추가해야한다(이 목록은 급속히 확장된다)

### 9.1.3 순환 피하기

- 로봇이 웹을 크롤링 할 때 루프나 순환에 빠지지 안도록 매우 조심해야 한다.
- 로봇들은 순환을 피하기 위해 반드시 그들이 어디를 방문 했는지를 알아야 한다.
- 순환은 로봇을 함정에 빠드려서 멈추게 하거나 진행을 느리게 한다.

### 9.1.4 루프와 중복

- 루프는 허술하게 설계 된 크롤러를 빙빙 돌게 만들 수 있고, 같은 페이지들을 반복해서 가져오는데 모든 시간을 허비하게 만들 수 있다.
- 크롤러가 같은 페이지를 반복해서 가져오면 고스란히 웹 서버의 부담이 된다.
- 중복된 콘테츠가 넘쳐나게 된다.

### 9.1.5 빵 부스러기의 흔적

- 방문한 곳을 지속적으로 추적하는 것은 쉽지 않다. 수억개의 URL은 빠른 검색 구조를 요구하기 때문에 빠른 속도가 중요하다.
- URL 목록의 완벽한 검색은 불가능하다.
- 로봇은 어떤 URL이 방문했던 곳인지 빠르게 결정하기 위해 적어도 검색 트리나 해시 테이블을 필요로 할 것 이다.
- **트리와 해시 테이블**
    - 복잡한 로봇이라면 방문한 URL을 추적하기 위해 검색 트리나 해시 테이블을 사용했을 수 도 있다.
- **느슨한 존재 비트맵**
    - 공간 사용을 최소화 하기 위해 , 몇몇 대규모 크롤러들은 존재 비트 배열과 같은 느슨한 자료구조를 사용한다.
- **체크포인트**
    - 로봇 프로그램이 갑작스럽게 중단될 경우를 대비해 , 방문한 URL의 목록이 디스크에 저장 되었는지 확인한다.
- **파티셔닝**
    - 각 로봇엔 URL들의 특정 “한 부분”이 할당되어 그에 대한 책임을 진다. 로봇들은 서로 도와 웹을 크롤링한다.

### 9.1.6 별칭(alias)과 로봇 순환

- 한 URL이 또 다른 URL에 대한 별칭이라면, 그 둘이 서로 달라 보이더라도 사실은 같은 리소스를 가리키고 있다.

### 9.1.7 URL 정규화하기

- URL들을 표준 형식으로 “정규화”함으로써 다른 URL과 같은 리소스를 가리키고 있음이 확실한 것들을 미리 제거 하려 시도한다.
1. 포트번호가 명시되지 않았다면, 호스트 명에 “:80”을 추가한다.
2. 모든 %xx 이스케이핑된 문자들을 대응되는 문자로 변환한다.
3. #태그들을 제거한다.

### 9.1.8 파일 시스템 링크 순환

![Untitled](9%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%8B%E1%85%B0%E1%86%B8%20%E1%84%85%E1%85%A9%E1%84%87%E1%85%A9%E1%86%BA%20b393c2da86494b2fb0756cfa4511632e/Untitled.png)

- 파일 시스템의 심벌릭 링크는 사실상 아무것도 존재하지 않으면서도 끊없이 깊어지는 디렉터리 계층을 만들 수 있기 때문에, 매우 교묘한 종류의 순환을 유발할 수 있다. 심벌릭 링크 순환은 서버 관리자가 실수로 만들게 되는 것이 보통이지만, 때때로 “사악한 웹 마스터”가 로봇을 함정에 빠뜨리기 위해 악의적으로 만들기도 한다.
- (a)의 경우
    - /index.html 을 가져와서, subdir/index.html 발견함.
    - subdir/index.html 가져와서, subdir/logo.gif로 이어지는 링크 발견
    - subdir/logo.gif을 가져오고 더 이상 링크가 없으므로 완료
- (b)의 경우
    - /index.html 을 가져와서, subdir/index.html 발견함.
    - subdir/index.html을 가져왔지만 같은 index.html로 되돌아감
    - subdir/subdir/index.html을 가져온다
    - subdir/subdir/subdir/index.html을 가져오고 무한반복한다.

### 9.1.9 동적 가상 웹 공간

- 의도적으로 복잡한 크롤러 루프를 만드는 것은 있을 수 있는 일이다. 특히 평범한 파일처럼 보이지만 사실은 게이트 웨이 애플리케이션인 URL을 만드는 것은 쉬운 일이다. 이 애플리케이션은 같은 서버에 있는 가상의 URL에 대한 링크를 포함한 HTML을 즉석에서 만들어 낼 수 있다.
- 가상 URL을 갖고 잇는 새 HTML을 페이지를 날조하여 만들어낸다.

### 9.1.10 루프와 중복 피하기

- 모든 순환을 피하는 완벽한 방법은 없다. 실제로 잘 서계된 로봇은 순환을 피하기 위해 휴리스틱의 집합을 필요로 한다.
- **URL 정규화**
    - URL을 표준 형태로 변환함으로써, 같은 리소스를 가리키는 중복된 URL이 생기는 것을 일부 회피한다.
- **너비 우선 크롤링**
    - URL들은 웹 사이트들 전체에 걸쳐 너비 우선으로 스케줄링하면, 순환의 영향을 최소화 할 수 있다.
- **스로틀링**
    - 로봇이 웹사이트에서 일정 시간 동안 가져올 수 있는 페이지의 숫자를 제한한다.
- **URL 크기 제한**
    - 로봇은 일정 길이(보통 1kb)를 넘는 URL의 크롤링은 거부할 수 있다.
- **URL/사이트 블랙 리스트**
    - 함정인 사이트의 URL의 목록을 만들어 관리하고 그들을 전염병 피하듯 피한다.
- **패턴 발견**
    - 몇몇 로봇은 반복되는 구성요소를 가진 URL을 잠재적인 순환으로 보고, 둘혹은 셋 이상의 반복된 구성요소를 가지고 있는 URL을 크롤링하는 것을 거절한다.
- **콘텐츠 지문**
    - 콘텐츠 지문을 사용하는 로봇들은 페이지의 콘텐츠에서 몇바이트를 얻어내어 체크섬으로 계산한다.
- **사람의 모니터링**
    - 크롤러 들은 문제를 예방 하기 위해 사람의 모니터링에 더욱 의존한다.

## 9.2 로봇의 HTTP

- 로봇들은 다른 HTTP 클라이언트 프로그램과 다르지 않다. 그 들 또한 HTTP 명세의 규칙을 지켜야한다. HTTP 요청을 만들고 스스로를 HTTP /1.1 클라이언트라고 광고 하는 로봇은 적절한 HTTP 요청 헤더를 사용해야 한다.
- 많은 로봇이 그들이 찾은 콘텐츠를 요청하기 위해 필요한 HTTP를 최소화 한으로만 구현 하려고한다.

### 9.2.1 요청 헤더 식별하기

- 로봇 개발자들이 구현을 하도록 권장 되는 기본적인 신원 식별 헤더들에는 아래와 같은 것이 있다.
    - User-Agent : 서버에게 요청을 만든 로봇의 이름을 말해준다.
    - From : 로봇의 사용자/관리자의 이메일 주소를 제공한다.
    - Accept : 서버에게 어떤 미디어 타입을 보내도 되는지 말해준다. 로봇이 관심있는 유형의 콘텐츠 만 받게 될 것임을 확실하는데 도움을 준다.
    - Referer : 현재의 요청 URL을 포함한 문서의 URL을 제공한다.

### 9.2.2 가상 호스트

- 로봇 구현자들은 Host 헤더를 지원할 필요가 있다. 가상 호스팅이 널리 퍼져있는 현실에서 , 요청에 Host 헤더를 포함하지 않으면 로봇이 어떤 URL에 대한 잘못된 콘텐츠를 찾게 만든다. 이러한 이유로 HTTP/1.1은 HOST헤더를 사용할 것을 요구한다.
- A와 B를 운영하는 서버에 로봇이 A에 페이지 요청에 HOST 헤더가 없이 호출한다고 하더라도
A에 대한 정보를 반환한다고 생각하지만 서버가 기본적으로 B를 제공하도록 되어있다면
A가 아닌 B를 반환하여 원하는 결과 값을 얻을 수 없다.

### 9.2.3 조건부 요청

- 오직 변경되었을 때만 콘텐츠를 가져오도록 하는 것을 의미한다.
- 시간이나 엔터티 태그를 비교함으로써 받아간 마지막 버전 이후에 업데이트 된 것이 있는지 알아보는 조건부 HTTP 요청을 구현한다.

### 9.2.4 응답 다루기

- 로봇은 대부분 Get 메서드로 콘텐츠를 요청해서 가져온다. 그러나 HTTP 의 특정 몇몇 기능을 사용하는 로봇드이나 웹탐색이나 서버와의 상호작용을 더 잘 해보려고 하는 로봇들을 여러종류의 HTTP 응답을 다룰 줄 알 필요가 있다.
- **상태코드**
    - 상태코드를 다룰 수 있어야 한다.
- **엔터티**
    - HTTP 헤더에 임베딩 된 정보를 따라 엔터티 자체의 정보를 찾을 수 있다.

### 9.2.5 User-Agent 타기팅

- 사이트 관리자들은 로봇의 요청을 다루기 위한 전략을 세워야 한다.
- 이트는 로봇에게 컨텐츠 대신 `에러 페이지`를 제공합니다. 사이트 관리자는 로봇의 요청을 다루기 위해서 `전략`을 세워 로봇이 방문하였을 때 컨텐츠가 없어 당황하는 일이 없도록 해야합니다.
- [http://www.useragentstring.com](http://www.useragentstring.com/)

## 9.3 부적절하게 동작하는 로봇들

로봇이 저지르는 **실수** 몇 가지와 그로 인해 초래되는 **결과**를 몇 가지를 알아보겠습니다.

- **오래된 URL**
    - 몇몇 로봇은 URL의 목록으로 방문한다. 그 목록이 오래되어서 에러 페이지를 제공한다면 웹서버에 대한 요청에 대한 능력이 감소된다.
- **길고 잘못된 URL**
    - URL이 길다면 이는 웹 서버의 처리 능력 영향을 주고 웹 서버의 접근 로그를 어지럽게 채우고, 심지어 허술한 웹 서버라면 고장을  일으킬 수 있다.
- **호기심이 지나친 로봇**
    - 웹에서 많은 양의 데이터를 검색하는 로봇의 구현자들은 사이트 구현자들이 인터넷 통해 접근 가능하게 만들 수 있다. 만약 그 데이터의 소유자가 원치 않는 웹페이지를 접근하여 부하가 생길 수 있다.

## 9.4 로봇 차단하기

- 로봇이 맞지 않는 장소에 들어오지 않도록 하고 웹 마스터에게 로봇의 동작을 더 잘 제어할 수 있는 메커니즘을 제공하는 단순하고 자발적인 기법이 제안되었다. 이 표준은 **“Robots Exclusion Standard”** 라고 이름 지어졌지만, 로봇의 접근을 제어하는 정보를 저장하는 파일의 이름을 따서 종종 그냥 robots.txt라고 불린다.
- 웹 서버는 서버의 문서 루트에 `robots.txt` 라고 이름 붙은 선택적인 파일을 제공함으로서 어떤 로봇이 서버의 어떤 부분에 **접근**할 수 있는지에 대한 **정보**가 담겨있습니다. robots.txt 파일은 로봇을 차단하지 않으므로 로봇은 그 페이지를 가져오게 됩니다.
- [robots.txt - 나무위키 (namu.wiki)](https://namu.wiki/w/robots.txt)
- [clien.net/robots.txt](https://www.clien.net/robots.txt)

### 9.4.1 로봇 차단 표준

- 로봇 차단 표준은 임시 방편으로 마련된 표준이다.
    - 0.0 : 로봇 배제 표준-Disallow 지시자를 지원하는 마틴 코스터의 오리지널 robots.txt 매커니즘 (1994.06)
    - 1.0 : 웹 로봇 제어 방법-Allow 지시자의 지원이 추가된 마틴 소크터의 IETF 초안 (1996.11)
    - 로봇 차단을 위한 확장 표준-정규식과 타이밍 정보를 포함한 숀 코너의 확장 (1996.11)

### 9.4.2 웹사이트와 robots.txt파일들

- 웹 사이트의 어떤 URL을 방문하기 전에 , 그 웹사이트 robots.txt 파일이 존재한다면 로봇은 반드시 그 파일을 가져와서 처리해야 한다.
- 로컬 robots.txt 파일을 웹사이트의 개별 서브 디렉터리에 설치할 수 있는 방법은 존재하지 않는다.
- 개발자는 웹사이트의 모든 콘텐츠에 대한 차단 규칙을 종합적으로 기술한 robots.txt 파일을 생성할 책임이 있다.
    - **robots.txt 가져오기** : HTTP GET 메소드를 통해 robots.txt 리소스를 가져오고, 만약 파일이 존재한다면 서버는 그 파일은 text/plain 본문으로 반환합니다. 만약 서버가 404 Not Found HTTP 상태 코드로 응답한다면 로봇의 접근을 제한하지 않는 것으로 간주하고 어떤 파일이든 요청하게 될 것입니다.
    - **응답코드**:
        - 서버가 성공(200) 으로 응답하면 로봇은 응답의 컨테츠를 파싱하여 차단 규칙을 얻고, 그 사이트에서 무언가를 가져오려 할 때 그 규칙을 따라야 합니다.
        - 리소스가 존재하지 않는다고 서버가 응답하면(404) 규칙이 존재하지 않는다고 판단하고 robots.txt에 제약없이 사이트에 접근 할 수 있습니다.
        - 서버가 접근 제한(401 혹은 403)으로 응답한다면 접근은 완전히 제한되어 있다고 가정합니다.
        - 일시적으로 실패했다면(503) 리소스 검색을 뒤로 미룹니다.
        - 리다이렉션이면(3XX) 리소스가 발견될 때 까지 리다이렉트를 따라갑니다.

### 9.4.3 robots.txt파일 포맷

- robots.txt파일은 매우 단순한 줄 기반 문서를 갖는다.
- robots.txt 파일의 각 줄은 **빈 줄, 주석 줄, 규칙 줄** 세 가지 종류가 있습니다. 규칙줄은 HTTP 헤더처럼 생겼고 (<필드>:<값>) 패턴 매칭을 위해 사용된다.
- User-Agent 줄: 로봇의 레코드는 하나 이상의 User-Agent 줄로 시작한다. 만약 로봇이 대응하는 User-Agent 줄을 찾지 못하였고 와일드 카드를 사용한 'User-Agent: *' 줄도 찾지 못했다면 접근에는 제한이 없습니다.
- Disallow 와 Allow 줄들: 이 줄들은 특정 로봇에 대해 어떤 URL 경로가 명시적으로 금지되어 있고 명시적으로 허용되는지 기술한다.

### 9.4.4 그 외에 알아둘 점

- 명세 발전에 따라 User-Agnet, Disallow, Allow 외에 다른 필드를 포함할 수 있습니다. 그리고 자신이 이해 못하는 필드는 무시합니다.
- 하위 호환성을 한 줄을 여러 줄로 나누어 적는 것은 비허용합니다.
- 주석은 파일의 어디에서든 허용됩니다.
- 로봇 차단 표전 버전 0.0은 Allow 줄을 지원하지 않았습니다.

### 9.4.5 robots.txt의 캐싱과 만료

- 매 접근마다 로봇이 robos.txt파일을 새로 가져와야 했다면 효율적이지 않다.
- 주기적인 robots.txt를 캐시 해야한다.
- 로봇은 HTTP 응답의 Cache-Control과 Expires 헤더에 주의를 기울여야 한다.

### 9.4.7 HTML 로봇제어 META 태그

- 로봇이 접근하는 것을 HTML 문서에서 직접 로봇 제어 태그를 추가할 수 있다.
- 로봇 차단 태그는 `HTML META 태그`를 이용해 다음과 같은 형식으로 구현됩니다.

```xml
<META NAME="ROBOTS" CONTENT="directive-list">
```

- `로봇 META 지시자`에 대해서 알아보겠습니다.
    - NOINDEX : 이 페이지를 처리하지 말고 무시하라고 전달합니다.
    - NOFOLLOW : 이 페이지가 링크한 페이지를 크롤링하지 말라고 전달합니다.
    - INDEX : 페이지의 컨텐츠를 인덱싱 허용합니다.
    - FOLLOW : 이 페이지가 링크한 페이지를 크롤링하라고 전달합니다.
    - NOARCHIVE : 이 페이지의 캐시를 위한 로컬 사본을 만들면 안 된다고 전달합니다.
    - ALL : INDEX, FOLLOW와 같습니다.
    - NONE : NOINDEX, NOFOLLOW와 같습니다.
    - 검색엔진 META 태그
        - DESCRIPTION : 저자가 웹 페이지의 짧은 요약을 정의합니다.
        - KEYWORDS : 키워드 검색을 돕기 위해 쉼표로 구별합니다.
        - REVISIT-AFTER : 쉽게 변경될 페이지들을 지정된 만큼의 날짜가 지난 이후에 다시 방문해야 한다고 지시합니다.

### 9.5로봇 에티켓

- 마틴 코스터는 웹 로봇을 만드는 사람들을 위한 가이드라인을 작성했다.
- [The Web Robots Pages (robotstxt.org)](https://www.robotstxt.org/guidelines.html)

### 9.6 현대적인 검색엔진의 아키텍처

- 웹로봇을 가장 광범위하게 사용한 것은 인터넷 검색 엔진이다. 오늘날 가장 유명한 웹사이트들의 상당수가 검색엔진이다.
- 웹 크롤러들은 마치 먹이를 주듯 검색 엔진에게 웹에 존재하는 문서들을 가져다 주어서, 검색엔진이 어떤문서에 어떤 단어들이 존재하는 대한 색인을 생성할 수 있게 한다.

### 9.6.1 넓게 생각하라

- 이전 검색엔진들은 사용자들의 검색을 위한 단순한 데이터베이스였다.
- 검색엔진은 수십억개의 웹페이지들을 검색하기 위해 크롤러를 사용해야한다.

### 9.6.2 현대적인 검색엔진의 아키텍처

- 오늘 날 검색엔진들은 그들이 갖고 있는 전 세계의 웹페이지들에 대한 풀 텍스트 색인이라고 하는 복잡한 로컬 데이터베이스를 생성한다. 이 색인은 웹의 모든 문서에서 일종의 카드 카탈로그처럼 동작한다.
- 크롤러(웹페이지 수집) → 풀 테스트 색인 추가 → 검색사이트들은 풀 텍스트 색에 대한 질의를 보낸다.
- 풀 텍스트 색인은 기껏해봐야 웹의 특정 순간에 대한 스냅숏에 불과하다.

### 9.6.3 풀 텍스트 색인

![Untitled](9%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%8B%E1%85%B0%E1%86%B8%20%E1%84%85%E1%85%A9%E1%84%87%E1%85%A9%E1%86%BA%20b393c2da86494b2fb0756cfa4511632e/Untitled%201.png)

- 풀 텍스트 색인은 단어 하나를 입력 받아 그 단어를 포함하고 있는 문서를 즉각 알려줄 수 있는 데이터베이스다.
- 문서들은 색인이 생성된 후에는 검색할 필요가 없다.

### 9.6.4 질의 보내기

- 사용자의 질의를 웹 검색 엔진 게이트웨이로 보내는 방법은 HTML 폼을 사용자가 채워 넣고 브라우저가 그 폼을 HTTP GET이나 POST 요청을 이용해서 게이트웨이로 보내는 식이다.

### 9.6.5 검색 결과를 정렬하고 보여주기

- 질의의 결과를 확인하기 위해 검색 엔진이 색인을 한번 사용했다면, 게이트웨이 애플리케이션은 그 결과를 이용해 최종 사용자를 위한 위한 결과 페이지를 즉석에서 만들어 낸다.
- 검색엔진은 그 문서들이 주어진 단어와 가장 관련이 많은 순서대로 결과 문서에 나타날 수 있도록 문서들 간의 순서를 알 필요가 있다.

### 9.6.6 스푸핑

- 스푸핑의 뜻은 속이다 라는 뜻을 가지고 있으며 많은 웹 마스터가 수 많은 키워드들을 나열한 가짜 페이지를 만들거나 , 특정 단어 대한 가짜 페이지를 생성하는 게이트웨이 애플리케이션을 만들어 사용한다.