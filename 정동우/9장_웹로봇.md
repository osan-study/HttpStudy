# 9. 웹 로봇

사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 프로그램으로 방식에 따라 `크롤러`, `스파이더`, `웜`, `봇` 등 다양한 이름으로 불린다.

웹 로봇의 몇가지 예:

- 주식시장 서버에 주기적으로 HTTP GET 요청을 보내고 주식 가격 변동을 확인하는 `주식 그래프 로봇`

- 월드 와이드 웹의 규모와 진화에 대한 통계 정보를 수집하는 `웹 통계 조사 로봇`으로 웹을 떠돌면서 페이지 개수를 세고 각 페이지의 크기, 언어, 미디어 타입을 기록한다.

- 검색 데이터베이스를 만들기 위해 발견한 모든 문서를 수집하는 `검색엔진 로봇`

- 상품에 대한 가격 데이터베이스를 만들기 위해 온라인 쇼핑몰의 카탈로그에서 웹 페이지를 수집하는 `가격 비교 로봇`

## 9.1 크롤러와 크롤링

웹 크롤러는 웹페이지를 하나 가져오고 그 페이지가 가리키는 모든 웹 페이지를 가져오는 일을 재귀적으로 반복하는 방식으로 웹을 순회하는 로봇이다. HTML 하이퍼링크들로 만들어진 웹을 따라 기어다니기<sup>crawl</sup> 때문에 `크롤러` 또는 `스파이더`라고 불린다.

인터넷 검색 엔진에서 크롤러를 사용하여 사용자들이 특정 단어를 포함한 문서를 찾을 수 있게 해준다.

### 9.1.1 어디에서 시작하는가: 루트 집합

크롤러가 방문을 시작하는 URL들의 초기 집합을 루트 집합<sup>root set</sup>이라고 불린다. 루트 집합에서 모든 링크를 크롤링 하면 관심 있는 웹페이지들의 대부분을 가져오게 되도록 충분히 다른 장소의 URL을 선택해야 한다.

좋은 루트 집합은 크고 인기 있는 웹사이트, 새로 생성된 페이지들의 목록, 잘 알려져 있지 않은 페이지들의 목록으로 구성되어 있다.

### 9.1.2 링크 추출과 상대 링크 정상화

크롤러는 검색한 각 페이지 안에 있는 URL을 파싱해서 크롤링 할 페이지 목록에 추가하고 상대링크를 절대 링크로 변환해야 한다.

### 9.1.3 순환 피하기

웹을 크롤링 할 때 루프에 빠지지 않도록 주의해야 하며 순환을 피하기 위해 어디를 방문했는지 알아야 한다.

### 9.1.4 루프와 중복

순환이 크롤러에게 해로운 이유:

- 순환에 빠지면 같은 페이지들을 반복해서 가져오는데 모든 시간을 허비하게 만들고 네트워크 대역폭을 낭비한다.

- 같은 페이지를 반복해서 가져오면 웹 서버에도 부담을 주게 된다.

- 중복된 컨텐츠를 보여주면 어플리케이션이 쓸모 없게 된다.

### 9.1.5 빵 부스러기의 흔적

어떤 URL을 방문했는지 판단하기 위해 복잡한 자료 구조를 사용해야 하고 속도와 메모리 사용 면에서 효율적이여야 한다.

#### 트리와 해시 테이블

URL을 훨씬 빨리 찾아 볼 수 있게 해주는 자료 구조다.

#### 느슨한 존재 비트맵

공간 사용을 최소화 하기 위해 몇몇 대규모 크롤러들은 존재 비트 배열<sup>presence bit array</sup>과 같은 느슨한 자료 구조를 사용한다.

URL은 해시 함수로 고정된 크기의 숫자로 변환되고 배열안에 대응하는 `존재 비트`<sup>presence bit</sup>가 이미 존재한다면 크롤러는 이 URL을 이미 크롤링 했다고 간주한다.

#### 체크포인트

로봇 프로그램이 갑자기 종료되는 경우에 대비해 URL 목록이 저장되었는지 확인한다.

#### 파티셔닝

몇몇 대규모 웹 로봇은 로봇들이 동시에 일하는 농장을 이용한다. 각 로봇엔 URL들의 특정한 부분이 할당되며 개별 로봇들은 URL들을 서로 넘겨주거나 오동작 하는 동료를 도와주거나 그 외의 이유로 활동을 조정하기 위해 커뮤니케이션 한다.

### 9.1.6 별칭<sup>alias</sup>과 로봇 순환

한 URL이 다른 URL의 별칭이라면 서로 달라보여도 같은 리소스를 가리키고 있다.

|     | URL                             | Alias                          | Comment                            |
| --- | ------------------------------- | ------------------------------ | ---------------------------------- |
| a   | http://www.foo.com/bar.html     | http://www.foo.com:80/bar.html | 기본포트 번호가 80일 때            |
| b   | http://www.foo.com/~bar.html    | http://www.foo.com/%7Fbar.html | %7F는 ~을 가리킨다.                |
| c   | http://www.foo.com/x.html#early | http://www.foo.com/x.html#late | 문서 내의 앵커가 서로 다를 때      |
| d   | http://www.foo.com/index.html   | http://www.foo.com/            | 기본 페이지가 index.html 일 때     |
| e   | http://www.foo.com/readme.html  | http://www.foo.com/README.html | 서버가 대소문자를 구분하지 않을 때 |
| f   | http://www.foo.com/             | http://123.456.789.123/        | www.foo.com이 이 IP 주소를 가질 때 |

### 9.1.7 URL 정규화하기

URL을 정규화하면 크롤러가 별칭을 인식하고 순환을 피할 수 있다.

- (a) 포트 번호가 명시되지 않았다면 :80을 추가한다.

- (b) 모든 %XX 이스케이프 시퀀스들을 해당하는 문자로 대체한다.

- (c) \# 태그들을 제거한다.

정규화 만으로 해결하기 어려운 문제는 다음과 같다:

- (d) 웹 서버가 대소문자를 구분하는지 식별이 필요하다.

- (e) 색인 페이지 설정을 알아야 한다.

- (f) 웹 서버가 가상 호스팅을 사용하는지 알아야 한다.

### 9.1.8 파일 시스템 링크 순환

파일 시스템의 심볼릭 링크는 끝없이 깊어지는 디렉터리 계층을 만들 수 있기 때문에 로봇이 무한한 순환을 일으킬 수 있다.

### 9.1.9 동적 가상 웹 공간

악의적인 웹에서는 가상의 URL 링크를 포함한 HTML을 만들어내서 로봇이 무한한 순환을 일으키게 할 수 있다.

### 9.1.10 루프와 중복 피하기

모든 순환을 피하는 완벽한 방법은 없으므로 휴리스틱한 방법을 사용하는데 이 경우 유효한 콘텐츠를 놓칠 수도 있다.

#### URL 정규화

URL을 표준 형태로 변환하여 중복 URL이 생기는 것을 일부 회피한다.

#### 너비 우선 크롤링

URL에 포함되어 있는 URL을 따라가지 않고 기존 URL 집합을 먼저 크롤링 한다.

#### 스로틀링

순환을 건드려서 특정 사이트의 별징들에 접근을 시도한다면 스로틀링을 이용해 서버에 대한 접근 횟수와 중복의 총 횟수를 제한 할 수 있다.

#### URL 크기 제한

일정 길이가 넘는 URL을 거부하여 순환을 중단하는 방법이다. 일부 가져오지 못하는 페이지가 있을 수 있지만 에러 로그를 남겨서 사용자에게 신호를 제공 할 수 있다.

#### URL/사이트 블랙리스트

로봇 순환을 만들거나 함정으로 알려진 사이트와 URL의 목록을 블랙리스트에 추가한다.

#### 패턴 발견

심볼릭 링크, 가상 URL에 의한 순환 패턴을 감지한다.

#### 콘텐츠 지문

페이지의 콘텐츠를 해시 함수를 사용해 지문으로 만들어서 중복을 감지한다. 동적으로 변경되는 페이지나 페이지 콘텐츠를 날짜, 카운터 등에 따라 다르게 표시하는 경우 제대로 동작하지 않을 수 있다.

#### 사람의 모니터링

진행 상황을 모니터링 할 수 있도록 진단과 로깅을 포함하여 특이한 일이나 문제가 발생되면 즉각 인지 할 수 있도록 설계 해야 한다.

## 9.2 로봇의 HTTP

로봇은 다른 HTTP 클라이언트 프로그램과 같은 방식으로 작동하므로 HTTP 명세의 규칙을 지켜야 한다.

### 9.2.1 요청 헤더 식별하기

로봇들은 약간의 신원 식별 헤더(User-Agent)를 구현하고 전송한다. 잘못된 크롤러의 소유자를 찾아내거나 로봇이 어떤 종류의 콘텐츠를 다룰 수 있는지에 대한 정보를 제곤한다.

권장되는 신원 식별 헤더는 다음과 같다.

- User-Agent: 로봇의 이름
- From: 로봇의 사용자/관리자의 이메일 주소
- Accept: 로봇이 가져올 수 있는 미디어 타입
- Referer: 현재의 요청 URL을 포함한 문서의 URL

### 9.2.2 가상 호스팅

요청에 Host 헤더를 포함하지 않으면 가상 호스팅으로 인해 잘못된 페이지를 가져올 수 있다.

두개 이상의 사이트를 운영하는 서버에 요청을 보냈을 때 Host 헤더를 요구하지 않는다면 어떤 사이트에서 가져온 문서인지 알 수 없다.

### 9.2.3 조건부 요청

로봇이 검색하는 컨텐츠의 양을 최소화 하기 위해 시간이나 엔터티 태그를 비교하고 기존에 가져간 마지막 버전 이후에 변경이 있을 경우 가져오도록 할 수 있다.

### 9.2.4 응답 다루기

대부분의 로봇들은 단순히 GET 메서드로 콘텐츠를 가져오기만 하지만 조건무 요청이나 서버와의 상호작용을 하는 로봇들은 여러종류의 HTTP 응답을 다룰 줄 알아야 한다.

#### 상태 코드

일반적인 상태코드나 예상할 수 있는 상태 코드를 다룰 수 있어야 한다. 모든 서버가 항상 적절한 에러 코드를 반환하지는 않지만 명세를 구현하기 위해 일단 알아둬야 한다.

#### 엔터티

메타 http-equiv 태그와 같은 HTML 태그는 리소스에 대해 컨텐츠 저자가 컨텐츠를 다루는 서버가 제공할 수도 있는 헤더를 덮어쓰기 위해 포함시킨 정보다.

```html
<meta http-equiv="Refresh" contnet="1; URL=index.html" />
```

위 태그는 수신자가 문서의 HTTP 응답 값이 "1; URL=index.html"인 Refresh HTTP 헤더를 포함하여 1초 후 index.html로 리다이렉트 되도록 지시한다.

### 9.2.5 User-Agent 타기팅

로봇이 사이트에 방문할 것을 전제하여 에러 메시지 대신 적절한 컨텐츠를 반환하도록 해야 한다.

## 9.3 부적절하게 동작하는 로봇들

#### 폭주하는 로봇

로봇이 너무 많은 요청을 보내서 서버에 과부하를 주는 것을 의미한다.

#### 오래된 URL

오래된 목록을 기반으로 작동하는 로봇은 더 이상 존재하지 않는 URL이나 서버에 요청을 보내는 경우가 있다.

#### 길고 잘못된 URL

프로그래밍 오류나 순환으로 인해 크고 의미없는 URL을 요청하게 되고 URL이 너무 길면 웹 서버의 처리 능력에 영향을 주고, 로그를 어지럽게 하고 허술한 사이트의 경우 고장을 일으킬 수 있다.

#### 호기심이 지나친 로봇

사적인 데이터에 대한 URL을 통해 소유자가 제공하지 않으려고 했던 정보를 긁어 오는 경우가 있으므로 민감한 데이터를 무시하는 메커니즘이 필요하다.

#### 동적 게이트웨이 접근

로봇이 게이트웨이 어플리케이션의 콘텐츠에 대한 URL로 요청하는 경우 특수 목적인 경우가 많고 처리 비용이 많이 든다.

## 9.4 로봇 차단하기

로봇의 동작을 제어할 수 있는 메커니즘을 제공하는 기법으로 `Robots Exclusion Standard`가 제안되었는데 보통 `robots.txt`라고 불린다.

서버의 문서 루트에 서버의 어떤 부분에 접근 가능한지에 대한 정보가 포함된 robots.txt 파일을 제공하면 로봇들은 이 파일을 읽어서 자신의 동작을 제어한다.

### 9.4.1 로봇 차단 표준

임시 방편으로 마련된 표준으로 표준의 소유한 주체가 없고 업체들이 제각각 구현하여 불완전한 부분이 있지만 없는 것 보다 낫기 때문에 대부분의 주류 업체들과 크롤러들은 이 차단 표준을 지원한다.
